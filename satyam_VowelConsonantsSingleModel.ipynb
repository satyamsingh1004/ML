{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset, random_split\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\n\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nimport cv2 as cv\n\nimport copy\n\nimport os\nfrom PIL import Image,ImageFilter\n\n\nprint('packages loaded!')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"class VowelConsonantDataset(Dataset):\n    def __init__(self, file_path,train=True,transform=None):\n        self.transform = transform\n        self.file_path=file_path\n        self.train=train\n        self.file_names=[file for _,_,files in os.walk(self.file_path) for file in files]\n        self.len = len(self.file_names)\n        if self.train:\n            self.classes_mapping=self.get_classes()\n    def __len__(self):\n        return len(self.file_names)\n    \n    def __getitem__(self, index):\n        file_name=self.file_names[index]\n        image_data=self.pil_loader(self.file_path+\"/\"+file_name)\n        #image_data = image_data.filter(ImageFilter.GaussianBlur(radius=5))\n        \n        if self.transform:\n            image_data = self.transform(image_data)\n        if self.train:\n            file_name_splitted=file_name.split(\"_\")\n            Y1 = self.classes_mapping[file_name_splitted[0]]\n            Y2 = self.classes_mapping[file_name_splitted[1]]\n            z1,z2=torch.zeros(10),torch.zeros(10)\n            z1[Y1-10],z2[Y2]=1,1\n            #label=torch.stack([z1,z2])\n            #_, label = torch.max(label.data, 1)\n           \n            label = torch.Tensor([Y1-10, Y2])             \n\n            return image_data, label\n\n        else:\n            return image_data, file_name\n          \n    def pil_loader(self,path):\n        with open(path, 'rb') as f:\n            img = Image.open(f)\n            return img.convert('RGB')\n\n      \n    def get_classes(self):\n        classes=[]\n        for name in self.file_names:\n            name_splitted=name.split(\"_\")\n            classes.extend([name_splitted[0],name_splitted[1]])\n        classes=list(set(classes))\n        classes_mapping={}\n        for i,cl in enumerate(sorted(classes)):\n            classes_mapping[cl]=i\n                \n        return classes_mapping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform= transforms.Compose([ \n    transforms.RandomAffine(10, translate=(0.1,0.1), scale=None, shear=None, resample=False, fillcolor=0),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                 std=[0.5, 0.5, 0.5])\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data = VowelConsonantDataset(\"../input/train/train\",train=True,transform=transform)\ntrain_size = int(0.9 * len(full_data))\ntest_size = len(full_data) - train_size\n\ntrain_data, validation_data = random_split(full_data, [train_size, test_size])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResNet50Bottom(nn.Module):\n    def __init__(self, original_model):\n        super(ResNet50Bottom, self).__init__()\n        self.features = nn.Sequential(*list(original_model.children())[:-1])\n        self.vowel = nn.Sequential(\n            nn.Linear(original_model.fc.in_features, num_classes),\n            nn.LeakyReLU(0.2, True)\n            #nn.LogSoftmax(dim = 1)\n        )\n        self.cons = nn.Sequential(\n            nn.Linear(original_model.fc.in_features, 10),\n            nn.LeakyReLU(0.2, True)\n           # nn.LogSoftmax(dim = 1)            \n        )\n        \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        v = self.vowel(x)\n        #_, v = torch.max(v.data, 1)\n        c = self.cons(x)\n        #_, c = torch.max(c.data, 1)\n        return v,c   \n      \n\nres50_model = models.resnet152(pretrained=True)\nfinal_model = ResNet50Bottom(res50_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in final_model.features.parameters():\n    param.requires_grad = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for param in final_model.parameters():\n    #if param.requires_grad:\n        #print(param.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\ntrainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\nvalidationloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size, shuffle=True)\ntest_data = VowelConsonantDataset(\"../input/test/test\",train=False,transform=transform)\ntestloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,shuffle=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = final_model.to(device)\nloss_fn = nn.CrossEntropyLoss()\nopt = optim.SGD(final_model.parameters(), lr=0.1)\n#opt = optim.RMSprop(final_model.parameters(), lr = 0.01, alpha = 0.9)\n#opt = optim.Adam(final_model.parameters(), lr = 0.01, betas= (0.9, 0.99))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_epoch_arr = []\nmax_epochs = 0\n\nmin_loss = 1000\n\nn_iters = np.ceil(train_size/batch_size)\n\nfor epoch in range(max_epochs):\n\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device, dtype=torch.int64)\n        \n        opt.zero_grad()\n\n        out_v,out_c = final_model(inputs)\n        loss_v = loss_fn(out_v,labels[:,:1].view(len(labels)))\n        loss_c = loss_fn(out_c,labels[:,1:2].view(len(labels)))\n        loss = loss_v + loss_c\n        loss.backward()\n        opt.step()\n        \n        if min_loss > loss.item():\n            min_loss = loss.item()\n            best_model = copy.deepcopy(final_model.state_dict())\n            print('Epoch:',epoch,'; Min loss %0.2f' % min_loss)\n        \n        if i % 100 == 0:\n            print('Iteration: %d/%d, Loss: %0.2f' % (i, n_iters, loss.item()))\n            \n        del inputs, labels, out_v,out_c\n        torch.cuda.empty_cache()\n        \n    loss_epoch_arr.append(loss.item())\n        \n   \n    \nplt.plot(loss_epoch_arr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Min loss %0.2f' % min_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluation(dataloader, model):\n    total, correct = 0, 0\n    for data in dataloader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device, dtype=torch.int64)\n        out_v,out_c = model(inputs)\n        _, pred_v = torch.max(out_v.data, 1)\n        _, pred_c = torch.max(out_c.data, 1)\n        total += labels.size(0)\n        \n        for i, (v, c) in enumerate(zip(pred_v, pred_c)):\n            if((v.item()==labels[i][0].item()) and (c.item()==labels[i][1].item())):\n                correct +=1\n            #else:\n                #print('Org Lbl: V',str(labels[i][0].item()) , '_C' , str(labels[i][1].item()), '; Pred lbl: V',str(v.item()) , '_C' + str(c.item()) )            \n    return 100 * correct / total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model.load_state_dict(best_model)\n#print(evaluation(trainloader, final_model), evaluation(validationloader, final_model))\n#print(evaluation(validationloader, final_model))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fulldataloader = torch.utils.data.DataLoader(full_data, batch_size=batch_size, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_epoch_arr = []\nmax_epochs = 40\n\nmin_loss = 1000\n\nn_iters = np.ceil(len(full_data)/batch_size)\n\nfor epoch in range(max_epochs):\n\n    for i, data in enumerate(fulldataloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device, dtype=torch.int64)\n        \n        opt.zero_grad()\n\n        out_v,out_c = final_model(inputs)\n        loss_v = loss_fn(out_v,labels[:,:1].view(len(labels)))\n        loss_c = loss_fn(out_c,labels[:,1:2].view(len(labels)))\n        loss = loss_v + loss_c\n        loss.backward()\n        opt.step()\n        \n        if min_loss > loss.item():\n            min_loss = loss.item()\n            best_model = copy.deepcopy(final_model.state_dict())\n            print('Min loss %0.2f' % min_loss)\n        \n        if i % 100 == 0:\n            print('Iteration: %d/%d, Loss: %0.2f' % (i, n_iters, loss.item()))\n            \n        del inputs, labels, out_v,out_c\n        torch.cuda.empty_cache()\n        \n    loss_epoch_arr.append(loss.item())\n        \n   \n    \nplt.plot(loss_epoch_arr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model.load_state_dict(best_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_ImageId = []\nfinal_Class= []\n\nfor data in testloader:\n    inputs, labels = data\n    inputs, labels = inputs.to(device), labels\n    out_v,out_c = final_model(inputs)\n    _, pred_v = torch.max(out_v.data, 1)\n    _, pred_c = torch.max(out_c.data, 1)\n    \n    for i, (v, c) in enumerate(zip(pred_v, pred_c)):\n        final_ImageId.append(labels[i])\n        final_Class.append('V' + str(v.item()) + '_C' + str(c.item()))      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = {}\nsubmission['ImageId'] = final_ImageId\nsubmission['Class'] = final_Class\nprint(final_ImageId)\nprint(final_Class)\n\nsubmission = pd.DataFrame(submission)\nsubmission = submission[['ImageId', 'Class']]\nsubmission = submission.sort_values(['ImageId'])\nsubmission.to_csv(\"submisision.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}