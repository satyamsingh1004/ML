{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activation Functions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyamsingh1004/ML/blob/master/Activation_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1qWylBIhHe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCfPbGMWhI-2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*  Although **sigmoid function** and itâ€™s derivative is simple and helps in reducing time required for making models, there is a major drawback of info loss due to the derivative having a short range. \n",
        "\n",
        "      The more layers there are in our Neural Network or the deeper our Neural Network is, the more info is compressed and lost at each layer and this amplifies and causes major data loss overall.\n",
        "\n",
        "\n"
      ]
    }
  ]
}